{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9af8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from db import get_user_data  # Your function to query DB\n",
    "\n",
    "user_profiles = {}\n",
    "\n",
    "def is_outdated(user_profile):\n",
    "    \"\"\"Returns True if the profile is outdated (e.g., older than 1 min).\"\"\"\n",
    "    return (datetime.now() - user_profile['updated_at']) > timedelta(minutes=1)\n",
    "\n",
    "def compute_profile(df):\n",
    "    \"\"\"Compute the user's typical location/time profile.\"\"\"\n",
    "    profile = {\n",
    "        \"avg_lat\": df['latitude'].mean(),\n",
    "        \"avg_lon\": df['longitude'].mean(),\n",
    "        \"common_hours\": df['hour'].mode().tolist(),\n",
    "        \"common_days\": df['day'].mode().tolist()\n",
    "    }\n",
    "    return profile\n",
    "\n",
    "def get_user_profile_cached(user_id):\n",
    "    \"\"\"Return the cached profile or recompute if missing/outdated.\"\"\"\n",
    "    if user_id not in user_profiles or is_outdated(user_profiles[user_id]):\n",
    "        df = get_user_data(user_id)\n",
    "        if df.empty:\n",
    "            return None\n",
    "        profile = compute_profile(df)\n",
    "        user_profiles[user_id] = {\n",
    "            \"profile\": profile,\n",
    "            \"updated_at\": datetime.now()\n",
    "        }\n",
    "    return user_profiles[user_id]['profile']\n",
    "\n",
    "\n",
    "from user_behavior import get_user_profile_cached\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "def is_anomalous(lat, lon, timestamp, user_id):\n",
    "    profile = get_user_profile_cached(user_id)\n",
    "    if not profile:\n",
    "        return False  # No data to compare against\n",
    "    \n",
    "    hour = timestamp.hour\n",
    "    day = timestamp.weekday()\n",
    "    \n",
    "    # Time anomaly\n",
    "    if hour not in profile['common_hours']:\n",
    "        print(\"Unusual hour\")\n",
    "\n",
    "    # Location anomaly\n",
    "    user_loc = (profile['avg_lat'], profile['avg_lon'])\n",
    "    current_loc = (lat, lon)\n",
    "    distance = geodesic(user_loc, current_loc).km\n",
    "    if distance > 1.0:\n",
    "        print(\"Unusual location\")\n",
    "\n",
    "    return (hour not in profile['common_hours']) or (distance > 1.0)\n",
    "\n",
    "# Main scoring\n",
    "risky_score, _, risky_type = predict_risk(lat, lon, method=\"dbscan\")\n",
    "is_behavior_anomaly = is_anomalous(lat, lon, timestamp, user_id)\n",
    "\n",
    "# Simple fusion logic\n",
    "final_score = risky_score\n",
    "if is_behavior_anomaly:\n",
    "    final_score += 0.3  # Boost if behavior looks weird\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c9d7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72be92b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fastapi'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 176\u001b[39m\n\u001b[32m    173\u001b[39m     update_user_profile(user_id, conn, new_data)\n\u001b[32m    175\u001b[39m \u001b[38;5;66;03m# API endpoint (assuming FastAPI)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[34;01mfastapi\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FastAPI\n\u001b[32m    178\u001b[39m app = FastAPI()\n\u001b[32m    180\u001b[39m \u001b[38;5;129m@app\u001b[39m.post(\u001b[33m\"\u001b[39m\u001b[33m/capture\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    181\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mcapture\u001b[39m(user_id: \u001b[38;5;28mstr\u001b[39m, lat: \u001b[38;5;28mfloat\u001b[39m, lon: \u001b[38;5;28mfloat\u001b[39m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'fastapi'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Configuration\n",
    "current_date = datetime(2025, 6, 28, 16, 13)  # 4:13 PM CET, Saturday, June 28, 2025\n",
    "distance_threshold = 0.05  # ~5.5 km for unusual location\n",
    "prob_threshold = 0.04  # 5% probability for unusual time\n",
    "late_night_hours = list(range(22, 24)) + list(range(0, 5))  # 10 PM - 5 AM\n",
    "min_data_points = 10  # Minimum data points for reliable user profile\n",
    "db_path = 'user_data.db'\n",
    "\n",
    "# Generate synthetic user data (fallback)\n",
    "def generate_synthetic_data(user_id, n_points=50):\n",
    "    # Generate synthetic data for a single user\n",
    "    if user_id == 'user1':\n",
    "        lat = np.random.normal(36.8065, 0.01, n_points // 2)  # Tunis (home)\n",
    "        lon = np.random.normal(10.1815, 0.01, n_points // 2)\n",
    "        lat = np.concatenate([lat, np.random.normal(36.8000, 0.01, n_points - n_points // 2)])  # Nearby (work)\n",
    "        lon = np.concatenate([lon, np.random.normal(10.1700, 0.01, n_points - n_points // 2)])\n",
    "    elif user_id == 'user2':\n",
    "        lat = np.random.normal(33.8815, 0.01, n_points)  # Gabes\n",
    "        lon = np.random.normal(10.0982, 0.01, n_points)\n",
    "    else:\n",
    "        lat = np.random.normal(35.6754, 0.01, n_points)  # Kairouan (default for new users)\n",
    "        lon = np.random.normal(10.1033, 0.01, n_points)\n",
    "    \n",
    "    synthetic_data = pd.DataFrame({\n",
    "        'user_id': [user_id] * n_points,\n",
    "        'latitude': lat,\n",
    "        'longitude': lon,\n",
    "        'timestamp': pd.date_range(start='2025-05-01', periods=n_points, freq='h'),\n",
    "    })\n",
    "    synthetic_data['hour'] = synthetic_data['timestamp'].dt.hour\n",
    "    synthetic_data['weekday'] = synthetic_data['timestamp'].dt.dayofweek\n",
    "    synthetic_data['month'] = synthetic_data['timestamp'].dt.month\n",
    "    return synthetic_data\n",
    "\n",
    "# Database setup and data retrieval\n",
    "def setup_database(data=None):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    if data is not None:\n",
    "        data.to_sql('user_locations', conn, if_exists='replace', index=False)\n",
    "    return conn\n",
    "\n",
    "def load_user_data(user_id, conn):\n",
    "    query = f\"SELECT * FROM user_locations WHERE user_id = '{user_id}'\"\n",
    "    try:\n",
    "        user_df = pd.read_sql(query, conn)\n",
    "        return user_df\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# Build user profile\n",
    "def build_user_profile(user_id, conn, fallback_data=None):\n",
    "    user_df = load_user_data(user_id, conn)\n",
    "    \n",
    "    # Fallback to synthetic data if insufficient\n",
    "    if user_df.empty or len(user_df) < min_data_points:\n",
    "        print(f\"Insufficient data for {user_id} (found {len(user_df)} points). Using synthetic data.\")\n",
    "        if fallback_data is None:\n",
    "            #user_df = generate_synthetic_data().query(f\"user_id == '{user_id}'\")\n",
    "            user_df = generate_synthetic_data(user_id)\n",
    "            if user_df.empty:\n",
    "                user_df = generate_synthetic_data(n_points=50)  # Ensure some data\n",
    "                user_df['user_id'] = user_id\n",
    "        else:\n",
    "            user_df = fallback_data.query(f\"user_id == '{user_id}'\")\n",
    "        # Update database with synthetic data\n",
    "        user_df.to_sql('user_locations', conn, if_exists='append', index=False)\n",
    "    \n",
    "    # Ensure timestamp is datetime\n",
    "    user_df['timestamp'] = pd.to_datetime(user_df['timestamp'], errors='coerce')\n",
    "    user_df['hour'] = user_df['timestamp'].dt.hour\n",
    "    user_df['weekday'] = user_df['timestamp'].dt.dayofweek\n",
    "    user_df['month'] = user_df['timestamp'].dt.month\n",
    "    \n",
    "    # Cluster user locations\n",
    "    user_locations = user_df[['latitude', 'longitude']].values\n",
    "    if len(user_locations) < 5:  # Minimum for clustering\n",
    "        print(f\"Not enough location points for {user_id} to cluster.\")\n",
    "        return None, None, None, None\n",
    "    \n",
    "    user_scaler = StandardScaler()\n",
    "    user_locations_scaled = user_scaler.fit_transform(user_locations)\n",
    "    user_optics = OPTICS(min_samples=5, xi=0.1, metric='euclidean')\n",
    "    user_clusters = user_optics.fit_predict(user_locations_scaled)\n",
    "    user_df['user_cluster'] = user_clusters\n",
    "    \n",
    "    # Frequent zones (centroids of non-noise clusters)\n",
    "    frequent_zones = user_df[user_df['user_cluster'] != -1].groupby('user_cluster')[['latitude', 'longitude']].mean()\n",
    "    \n",
    "    # Usual activity times\n",
    "    usual_hours = user_df['hour'].value_counts(normalize=True)\n",
    "    usual_weekdays = user_df['weekday'].value_counts(normalize=True)\n",
    "    usual_months = user_df['month'].value_counts(normalize=True)\n",
    "    \n",
    "    return frequent_zones, usual_hours, usual_weekdays, usual_months\n",
    "\n",
    "\n",
    "\n",
    "# Detect anomalies\n",
    "def detect_user_anomalies(lat, lon, hour, weekday, month, user_id, conn):\n",
    "    frequent_zones, usual_hours, usual_weekdays, usual_months = build_user_profile(user_id, conn)\n",
    "    \n",
    "    if frequent_zones is None:\n",
    "        print(f\"No profile for {user_id}. Assuming no anomalies.\")\n",
    "        return 0.0, 0.0\n",
    "    # Load the user data to compute historical distances\n",
    "    user_df = load_user_data(user_id, conn)\n",
    "    if not user_df.empty:\n",
    "        historical_distances = np.sqrt(((user_df['latitude'] - lat) ** 2 + (user_df['longitude'] - lon) ** 2))\n",
    "        distance_threshold = np.percentile(historical_distances, 95) if len(historical_distances) > 0 else 0.05\n",
    "    else:\n",
    "        distance_threshold = 0.05  # Default if no data\n",
    "    \n",
    "    # Unusual location\n",
    "    location_anomaly = 0.0\n",
    "    min_distance = np.inf\n",
    "    if not frequent_zones.empty:\n",
    "        for _, zone in frequent_zones.iterrows():\n",
    "            distance = np.sqrt((lat - zone['latitude'])**2 + (lon - zone['longitude'])**2)\n",
    "            min_distance = min(min_distance, distance)\n",
    "        if min_distance > distance_threshold:\n",
    "            location_anomaly = 1.0\n",
    "    print(f\"Location anomaly score: {location_anomaly:.2f} (min distance: {min_distance:.2f})\")\n",
    "\n",
    "    # Unusual time\n",
    "    time_anomaly = 0.0\n",
    "    hour_prob = usual_hours.get(hour, 0.01)\n",
    "    weekday_prob = usual_weekdays.get(weekday, 0.01)\n",
    "    month_prob = usual_months.get(month, 0.01)\n",
    "    if hour_prob < prob_threshold:\n",
    "        time_anomaly += 0.5\n",
    "    if weekday_prob < prob_threshold:\n",
    "        time_anomaly += 0.3\n",
    "    if month_prob < prob_threshold:\n",
    "        time_anomaly += 0.15\n",
    "    if hour in late_night_hours:\n",
    "        time_anomaly += 0.5\n",
    "    time_anomaly = min(time_anomaly, 1.0)\n",
    "    print(f\"Time anomaly score: {time_anomaly:.2f} (hour prob: {hour_prob:.2f}, weekday prob: {weekday_prob:.2f}, month prob: {month_prob:.2f})\")\n",
    "\n",
    "    return location_anomaly, time_anomaly\n",
    "\n",
    "\n",
    "# New function to update database and profile\n",
    "def update_user_profile(user_id, conn, new_data):\n",
    "    # Append new data to the database\n",
    "    new_data.to_sql('user_locations', conn, if_exists='append', index=False)\n",
    "    print(f\"Updated database with new data for {user_id} at {new_data['timestamp'].iloc[0]}\")\n",
    "    \n",
    "    # Optional: Rebuild profile immediately (commented out for performance)\n",
    "    frequent_zones, usual_hours, usual_weekdays, usual_months, user_scaler = build_user_profile(user_id, conn)\n",
    "\n",
    "# Capture and store new data\n",
    "def capture_and_store(user_id, latitude, longitude, conn):\n",
    "    now = datetime.now()\n",
    "    new_data = pd.DataFrame([{\n",
    "        'user_id': user_id,\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'timestamp': now,\n",
    "        'hour': now.hour,\n",
    "        'weekday': now.weekday(),\n",
    "        'month': now.month\n",
    "    }])\n",
    "    update_user_profile(user_id, conn, new_data)\n",
    "\n",
    "# API endpoint (assuming FastAPI)\n",
    "from fastapi import FastAPI\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/capture\")\n",
    "def capture(user_id: str, lat: float, lon: float):\n",
    "    conn = sqlite3.connect('user_data.db')\n",
    "    try:\n",
    "        capture_and_store(user_id, lat, lon, conn)\n",
    "        return {\"status\": \"success\"}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "# Example periodic profile update (using a simple loop for demo)\n",
    "import time\n",
    "\n",
    "def periodic_profile_update():\n",
    "    conn = sqlite3.connect('user_data.db')\n",
    "    while True:\n",
    "        for user_id in ['user1', 'user2', 'user3']:  # Add dynamic user list if needed\n",
    "            build_user_profile(user_id, conn)\n",
    "        print(f\"Profiles updated at {datetime.now()}\")\n",
    "        time.sleep(3600)  # Update every hour (3600 seconds)\n",
    "\n",
    "\n",
    "def process_new_user_data(user_id, new_data, conn):\n",
    "    # Step 1: Store the new data\n",
    "    new_data.to_sql('user_locations', conn, if_exists='append', index=False)\n",
    "    print(f\"✅ New data stored for user: {user_id}\")\n",
    "\n",
    "    # Step 2: Rebuild profile (frequent zones + usual hours)\n",
    "    profile = build_user_profile(user_id, conn)\n",
    "    if profile is None:\n",
    "        print(\"⚠️ Could not build profile.\")\n",
    "        return None\n",
    "    \n",
    "    frequent_zones, usual_hours, usual_weekdays, usual_months = profile\n",
    "\n",
    "    # Step 3: Detect anomalies for the new data points\n",
    "    results = []\n",
    "    for _, row in new_data.iterrows():\n",
    "        lat = row['latitude']\n",
    "        lon = row['longitude']\n",
    "        timestamp = pd.to_datetime(row['timestamp'])\n",
    "        hour = timestamp.hour\n",
    "        weekday = timestamp.dayofweek\n",
    "        month = timestamp.month\n",
    "\n",
    "        loc_anomaly, time_anomaly = detect_user_anomalies(\n",
    "            lat, lon, hour, weekday, month, user_id, conn\n",
    "        )\n",
    "\n",
    "        results.append({\n",
    "            'user_id': user_id,\n",
    "            'timestamp': timestamp,\n",
    "            'location_anomaly': loc_anomaly,\n",
    "            'time_anomaly': time_anomaly,\n",
    "            'total_risk_score': round((loc_anomaly + time_anomaly) / 2, 2)\n",
    "        })\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize database\n",
    "    conn = setup_database() # SQLite connection\n",
    "    \n",
    "    # Optional: Initialize with synthetic data if database is empty\n",
    "    try:\n",
    "        test_df = pd.read_sql(\"SELECT * FROM user_locations LIMIT 1\", conn)\n",
    "        if test_df.empty:\n",
    "            #synthetic_data = generate_synthetic_data()\n",
    "            synthetic_data = pd.concat([generate_synthetic_data('user1'), generate_synthetic_data('user2')])\n",
    "            synthetic_data.to_sql('user_locations', conn, if_exists='append', index=False)\n",
    "    except:\n",
    "        synthetic_data = generate_synthetic_data()\n",
    "        synthetic_data.to_sql('user_locations', conn, if_exists='append', index=False)\n",
    "    \n",
    "    # Test anomaly detection\n",
    "    test_points = [\n",
    "        ('user1', 36.8065, 10.1815, 16, 5, 6),  # User1, Tunis (home), 4 PM, Saturday, June\n",
    "        ('user1', 33.8815, 10.0982, 23, 5, 6),  # User1, Gabes (unusual), 11 PM (unusual), Saturday, June\n",
    "        ('user2', 33.8815, 10.0982, 16, 5, 6),  # User2, Gabes (home), 4 PM, Saturday, June\n",
    "        ('user2', 36.8065, 10.1815, 2, 5, 2),   # User2, Tunis (unusual), 2 AM (unusual), February\n",
    "        ('user3', 35.6754, 10.1033, 3, 4, 2)    # User3, Kairouan (no data), 3 AM, Friday, February\n",
    "    ]\n",
    "    \n",
    "    for user_id, lat, lon, hour, weekday, month in test_points:\n",
    "        loc_anomaly, time_anomaly = detect_user_anomalies(lat, lon, hour, weekday, month, user_id, conn)\n",
    "        print(f\"Anomaly for {user_id} at ({lat}, {lon}, {hour}:00, Weekday: {weekday}, Month: {month}): \"\n",
    "            f\"Location Anomaly: {loc_anomaly:.2f}, Time Anomaly: {time_anomaly:.2f}\")\n",
    "    # Visualize user locations\n",
    "    user_data = pd.read_sql(\"SELECT * FROM user_locations\", conn)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(data=user_data, x='longitude', y='latitude', hue='user_id', size=1, alpha=0.5)\n",
    "    plt.title('User Locations from Database')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Start periodic update in background (for demo; use a proper scheduler in production)\n",
    "    import threading\n",
    "    update_thread = threading.Thread(target=periodic_profile_update, daemon=True)\n",
    "    update_thread.start()\n",
    "\n",
    "    # Run the FastAPI app (for testing; use uvicorn in production)\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "\n",
    "\n",
    "\n",
    "    # Example new user capture (e.g. from frontend)\n",
    "    from datetime import datetime\n",
    "    new_data = pd.DataFrame([{\n",
    "        'user_id': 'user1',\n",
    "        'latitude': 36.8211,\n",
    "        'longitude': 10.2044,\n",
    "        'timestamp': datetime.now()\n",
    "    }])\n",
    "    alerts = process_new_user_data('user1', new_data, conn)\n",
    "\n",
    "    for alert in alerts:\n",
    "        print(alert)\n",
    "\n",
    "    \n",
    "    # Close database connection\n",
    "    conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48af3798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_risk_with_anomalies(lat, lon, hour, weekday, month, user_id, conn, model, scaler, cluster_weights, cluster_fatalities, cluster_event_types, cluster_temporal_density, X_scaled, low_risk_clusters, beta=2.0):\n",
    "    loc_anomaly, time_anomaly = detect_user_anomalies(lat, lon, hour, weekday, month, user_id, conn)\n",
    "    X_new = np.array([[lat, lon]], dtype=np.float32)\n",
    "    X_new_scaled = scaler.transform(X_new)\n",
    "    core_indices = np.where(model.core_distances_ != np.inf)[0]\n",
    "    if len(core_indices) == 0:\n",
    "        return 0.0, 0.0, 'None', loc_anomaly, time_anomaly\n",
    "    core_points = X_scaled[core_indices]\n",
    "    labels = model.labels_[core_indices]\n",
    "    distances = np.sqrt(((X_new_scaled - core_points) ** 2).sum(axis=1))\n",
    "    nearest_idx = np.argmin(distances)\n",
    "    normalized_distance = distances[nearest_idx] / np.sqrt(((X_scaled - X_scaled.mean(axis=0)) ** 2).sum(axis=1)).max()\n",
    "    nearest_cluster = labels[nearest_idx]\n",
    "    event_type = cluster_event_types.get(nearest_cluster, 'Unknown')\n",
    "    eps = np.percentile(model.core_distances_[core_indices], 95) if len(core_indices) > 0 else 0.05\n",
    "    if distances[nearest_idx] <= eps:\n",
    "        spatial_score = cluster_weights.get(nearest_cluster, 0) / (cluster_weights.max() if not cluster_weights.empty else 1)\n",
    "        spatial_score *= (1 + 0.2 * cluster_fatalities.get(nearest_cluster, 0) + 0.3 * cluster_temporal_density.get(nearest_cluster, 0))\n",
    "        spatial_score /= (cluster_weights / cluster_weights.max() * (1 + 0.2 * cluster_fatalities + 0.3 * cluster_temporal_density)).max()\n",
    "    else:\n",
    "        base_score = cluster_weights.get(nearest_cluster, 0) / (cluster_weights.max() if not cluster_weights.empty else 1)\n",
    "        base_score *= (1 + 0.2 * cluster_fatalities.get(nearest_cluster, 0) + 0.3 * cluster_temporal_density.get(nearest_cluster, 0))\n",
    "        spatial_score = max(base_score * np.exp(-beta * (distances[nearest_idx] - eps)), 0.2 * base_score)\n",
    "        spatial_score /= (cluster_weights / cluster_weights.max() * (1 + 0.2 * cluster_fatalities + 0.3 * cluster_temporal_density)).max()\n",
    "    if nearest_cluster in low_risk_clusters:\n",
    "        loc_anomaly = 0.0\n",
    "    combined_score = spatial_score * (1 + 0.5 * loc_anomaly + 0.5 * time_anomaly)\n",
    "    combined_score = min(1.0, combined_score)\n",
    "    return combined_score, normalized_distance, event_type, loc_anomaly, time_anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b57f5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from fastapi import FastAPI\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "\n",
    "app = FastAPI()\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['hydatis']\n",
    "collection = db['user_locations']\n",
    "collection.create_index([(\"latitude\", \"2dsphere\"), (\"longitude\", \"2dsphere\")])\n",
    "\n",
    "def detect_user_anomalies(lat, lon, hour, weekday, month, user_id, collection):\n",
    "    # Placeholder; implement full logic\n",
    "    return 0.0, 0.0\n",
    "\n",
    "def build_user_profile(user_id, collection):\n",
    "    # Placeholder; implement full logic\n",
    "    return None, None, None, None, None\n",
    "\n",
    "def capture_and_store_mongodb(user_id, latitude, longitude, collection):\n",
    "    now = datetime.now()\n",
    "    new_data = pd.DataFrame([{\n",
    "        'user_id': user_id,\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'timestamp': now,\n",
    "        'hour': now.hour,\n",
    "        'weekday': now.weekday(),\n",
    "        'month': now.month\n",
    "    }])\n",
    "    collection.insert_many(new_data.to_dict('records'))\n",
    "    print(f\"Captured data for {user_id} at {now}\")\n",
    "\n",
    "def trigger_alert(user_id, lat, lon, loc_anomaly, time_anomaly):\n",
    "    print(f\"ALERT for {user_id} at ({lat}, {lon}): Location Anomaly {loc_anomaly:.2f}, Time Anomaly {time_anomaly:.2f}\")\n",
    "    # Integrate with emergency services API here\n",
    "\n",
    "def sync_offline_data(user_id, collection, local_data):\n",
    "    if not local_data.empty:\n",
    "        collection.insert_many(local_data.to_dict('records'))\n",
    "        print(f\"Synced offline data for {user_id}\")\n",
    "\n",
    "@app.post(\"/capture\")\n",
    "def capture(user_id: str, lat: float, lon: float, emergency: bool = False):\n",
    "    try:\n",
    "        capture_and_store_mongodb(user_id, lat, lon, collection)\n",
    "        loc_anomaly, time_anomaly = detect_user_anomalies(lat, lon, datetime.now().hour, datetime.now().weekday(), datetime.now().month, user_id, collection)\n",
    "        if (loc_anomaly > 0.5 or time_anomaly > 0.5) or emergency:\n",
    "            trigger_alert(user_id, lat, lon, loc_anomaly, time_anomaly)\n",
    "        return {\"status\": \"success\"}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "@app.post(\"/sync\")\n",
    "def sync(user_id: str, local_data: list):  # Expect JSON array from client\n",
    "    try:\n",
    "        df = pd.DataFrame(local_data)\n",
    "        sync_offline_data(user_id, collection, df)\n",
    "        return {\"status\": \"success\"}\n",
    "    except Exception as e:\n",
    "        return {\"status\": \"error\", \"message\": str(e)}\n",
    "\n",
    "def periodic_update():\n",
    "    for user_id in collection.distinct('user_id'):\n",
    "        build_user_profile(user_id, collection)\n",
    "        threshold = datetime.now() - pd.Timedelta(days=30)\n",
    "        collection.delete_many({'timestamp': {'$lt': threshold}, 'user_id': user_id})\n",
    "    print(f\"Profiles updated at {datetime.now()}\")\n",
    "\n",
    "scheduler = BackgroundScheduler()\n",
    "scheduler.add_job(periodic_update, 'interval', minutes=30)\n",
    "scheduler.start()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97267ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import OPTICS\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from fastapi import FastAPI\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "\n",
    "app = FastAPI()\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['hydatis']\n",
    "collection = db['user_locations']\n",
    "collection.create_index([(\"latitude\", \"2dsphere\"), (\"longitude\", \"2dsphere\")])\n",
    "\n",
    "# Constants\n",
    "distance_threshold = 0.05\n",
    "prob_threshold = 0.05\n",
    "late_night_hours = list(range(22, 24)) + list(range(0, 5))\n",
    "\n",
    "# Build profile per user\n",
    "def build_user_profile(user_id, collection):\n",
    "    df = pd.DataFrame(list(collection.find({'user_id': user_id})))\n",
    "    if df.empty or len(df) < 10:\n",
    "        return None, None, None, None, None\n",
    "\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "    df['hour'] = df['timestamp'].dt.hour\n",
    "    df['weekday'] = df['timestamp'].dt.dayofweek\n",
    "    df['month'] = df['timestamp'].dt.month\n",
    "\n",
    "    # Cluster locations\n",
    "    locations = df[['latitude', 'longitude']].values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(locations)\n",
    "    optics = OPTICS(min_samples=5, xi=0.1)\n",
    "    labels = optics.fit_predict(X_scaled)\n",
    "    df['cluster'] = labels\n",
    "\n",
    "    centroids = df[df['cluster'] != -1].groupby('cluster')[['latitude', 'longitude']].mean()\n",
    "    hour_freq = df['hour'].value_counts(normalize=True)\n",
    "    weekday_freq = df['weekday'].value_counts(normalize=True)\n",
    "    month_freq = df['month'].value_counts(normalize=True)\n",
    "\n",
    "    return centroids, hour_freq, weekday_freq, month_freq, scaler\n",
    "\n",
    "# Detect anomalies\n",
    "\n",
    "def detect_user_anomalies(lat, lon, hour, weekday, month, user_id, collection):\n",
    "    profile = build_user_profile(user_id, collection)\n",
    "    if profile[0] is None:\n",
    "        return 0.0, 0.0\n",
    "\n",
    "    centroids, hour_freq, weekday_freq, month_freq, scaler = profile\n",
    "\n",
    "    # Location anomaly\n",
    "    loc_anomaly = 0.0\n",
    "    point = np.array([[lat, lon]])\n",
    "    for _, zone in centroids.iterrows():\n",
    "        dist = np.sqrt((lat - zone['latitude'])**2 + (lon - zone['longitude'])**2)\n",
    "        if dist < distance_threshold:\n",
    "            break\n",
    "    else:\n",
    "        loc_anomaly = 1.0\n",
    "\n",
    "    # Time anomaly\n",
    "    time_anomaly = 0.0\n",
    "    hour_prob = hour_freq.get(hour, 0.01)\n",
    "    weekday_prob = weekday_freq.get(weekday, 0.01)\n",
    "    month_prob = month_freq.get(month, 0.01)\n",
    "\n",
    "    if hour_prob < prob_threshold:\n",
    "        time_anomaly += 0.5\n",
    "    if weekday_prob < prob_threshold:\n",
    "        time_anomaly += 0.3\n",
    "    if month_prob < prob_threshold:\n",
    "        time_anomaly += 0.2\n",
    "    if hour in late_night_hours:\n",
    "        time_anomaly += 0.5\n",
    "\n",
    "    time_anomaly = min(time_anomaly, 1.0)\n",
    "    return loc_anomaly, time_anomaly\n",
    "\n",
    "# Store new point\n",
    "def capture_and_store(user_id, latitude, longitude):\n",
    "    now = datetime.now()\n",
    "    data = {\n",
    "        'user_id': user_id,\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'timestamp': now,\n",
    "        'hour': now.hour,\n",
    "        'weekday': now.weekday(),\n",
    "        'month': now.month\n",
    "    }\n",
    "    collection.insert_one(data)\n",
    "\n",
    "# Trigger\n",
    "\n",
    "def trigger_alert(user_id, lat, lon, loc_a, time_a):\n",
    "    print(f\"\\nALERT: {user_id} at ({lat}, {lon})\\nLocation anomaly: {loc_a:.2f}, Time anomaly: {time_a:.2f}\\n\")\n",
    "\n",
    "# FastAPI endpoint\n",
    "@app.post(\"/capture\")\n",
    "def capture(user_id: str, lat: float, lon: float, emergency: bool = False):\n",
    "    capture_and_store(user_id, lat, lon)\n",
    "    loc_a, time_a = detect_user_anomalies(lat, lon, datetime.now().hour, datetime.now().weekday(), datetime.now().month, user_id, collection)\n",
    "    if (loc_a > 0.5 or time_a > 0.5) or emergency:\n",
    "        trigger_alert(user_id, lat, lon, loc_a, time_a)\n",
    "    return {\"status\": \"success\", \"loc_anomaly\": loc_a, \"time_anomaly\": time_a}\n",
    "\n",
    "@app.post(\"/sync\")\n",
    "def sync(user_id: str, local_data: list):\n",
    "    df = pd.DataFrame(local_data)\n",
    "    df['user_id'] = user_id\n",
    "    collection.insert_many(df.to_dict('records'))\n",
    "    return {\"status\": \"synced\"}\n",
    "\n",
    "# Background profile update\n",
    "\n",
    "def periodic_update():\n",
    "    for uid in collection.distinct(\"user_id\"):\n",
    "        build_user_profile(uid, collection)\n",
    "        collection.delete_many({\"timestamp\": {\"$lt\": datetime.now() - pd.Timedelta(days=30)}, \"user_id\": uid})\n",
    "    print(\"[INFO] Profiles updated.\")\n",
    "\n",
    "scheduler = BackgroundScheduler()\n",
    "scheduler.add_job(periodic_update, 'interval', minutes=30)\n",
    "scheduler.start()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
