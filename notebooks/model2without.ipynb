{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d9420109",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'packaging.requirements'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspeechbrain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpretrained\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EncoderClassifier\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyannote\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Pipeline\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwhisper\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mre\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyannote/audio/__init__.py:29\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Inference\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pyannote/audio/core/inference.py:33\u001b[39m\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01meinops\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rearrange\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyannote\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Segment, SlidingWindow, SlidingWindowFeature\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorch_lightning\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutilities\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmemory\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_oom_error\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyannote\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioFile\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpyannote\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodel\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Model, Specifications\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/pytorch_lightning/__init__.py:6\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlogging\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning_utilities\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_available\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.isfile(os.path.join(os.path.dirname(\u001b[34m__file__\u001b[39m), \u001b[33m\"\u001b[39m\u001b[33m__about__.py\u001b[39m\u001b[33m\"\u001b[39m)):\n\u001b[32m      9\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpytorch_lightning\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m__about__\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/lightning_utilities/__init__.py:8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning_utilities\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply_func\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m apply_to_collection\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning_utilities\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menums\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StrEnum\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning_utilities\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compare_version, module_available\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning_utilities\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moverrides\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_overridden\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning_utilities\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrank_zero\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WarningCache\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/lightning_utilities/core/__init__.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning_utilities\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapply_func\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m apply_to_collection\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning_utilities\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01menums\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StrEnum\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning_utilities\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mimports\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m compare_version, module_available\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning_utilities\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01moverrides\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m is_overridden\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightning_utilities\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcore\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrank_zero\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WarningCache\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/python/3.12.1/lib/python3.12/site-packages/lightning_utilities/core/imports.py:16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ModuleType\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Any, Callable, Optional, TypeVar\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrequirements\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Requirement\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m InvalidVersion, Version\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ParamSpec\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'packaging.requirements'"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "import librosa\n",
    "import numpy as np\n",
    "import torch\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from pyannote.audio import Pipeline\n",
    "import whisper\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86e77989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All required libraries are installed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "speechbrain.lobes.models.huggingface_transformers.huggingface - Wav2Vec2Model is frozen.\n",
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/pytorch_lightning/utilities/migration/migration.py:208: You have multiple `ModelCheckpoint` callback states in this checkpoint, but we found state keys that would end up colliding with each other after an upgrade, which means we can't differentiate which of your checkpoint callbacks needs which states. At least one of your `ModelCheckpoint` callbacks will not be able to reload the state.\n",
      "Lightning automatically upgraded your loaded checkpoint from v1.1.3 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../home/codespace/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/059e96f964841d40f1a5e755bb7223f76666bba4/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.7.1, yours is 2.7.1+cu126. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.5.1.post0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../../../home/codespace/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.7.1+cu126. Bad things might happen unless you revert torch to 1.x.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/python/3.12.1/lib/python3.12/site-packages/speechbrain/utils/autocast.py:188: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  wrapped_fwd = torch.cuda.amp.custom_fwd(fwd, cast_inputs=cast_inputs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import whisper\n",
    "# Install libraries if not present (run this cell first)\n",
    "from speechbrain.pretrained import EncoderClassifier\n",
    "from pyannote.audio import Pipeline\n",
    "\n",
    "\n",
    "def install_libraries():\n",
    "    required = {'speechbrain', 'pyannote.audio', 'openai-whisper', 'librosa', 'torch', 'numpy'}\n",
    "    installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "    missing = required - installed\n",
    "\n",
    "    if missing:\n",
    "        print(\"Installing missing libraries...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", *missing])\n",
    "    else:\n",
    "        print(\"All required libraries are installed.\")\n",
    "\n",
    "try:\n",
    "    import pkg_resources\n",
    "    install_libraries()\n",
    "except ImportError:\n",
    "    print(\"Please install pkg_resources or run pip install manually.\")\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"speechbrain\", \"pyannote.audio\", \"openai-whisper\", \"librosa\", \"torch\", \"numpy\"])\n",
    "\n",
    "# Load pre-trained models\n",
    "try:\n",
    "    transcription_model = whisper.load_model(\"base\")  # Fallback; replace with Tunisian ASR if available\n",
    "    stress_model = EncoderClassifier.from_hparams(source=\"speechbrain/emotion-recognition-wav2vec2-IEMOCAP\", savedir=\"pretrained_models/emotion\")\n",
    "    tone_pipeline = Pipeline.from_pretrained(\"pyannote/voice-activity-detection\", use_auth_token=\"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\")\n",
    "    rhythm_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization\", use_auth_token=\"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\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading models: {e}. Please ensure libraries are installed and token is set.\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083ef21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "# Function to analyze vocal audio\n",
    "def analyze_vocal_audio(audio_file, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Analyzes audio for stress, tone, rhythm, and transcription for personal security alerts.\n",
    "    \n",
    "    Args:\n",
    "        audio_file (str): Path to the audio file (WAV, 16kHz, mono).\n",
    "        sample_rate (int): Audio sample rate (default: 16000 Hz).\n",
    "    \n",
    "    Returns:\n",
    "        dict: Results including transcription, stress, tone, rhythm, and alert level.\n",
    "    \"\"\"\n",
    "    # Load audio\n",
    "    try:\n",
    "        audio, sr = librosa.load(audio_file, sr=sample_rate)\n",
    "        if sr != sample_rate:\n",
    "            audio = librosa.resample(audio, orig_sr=sr, target_sr=sample_rate)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Audio file '{audio_file}' not found. Please provide a WAV file or record one.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading audio: {e}\")\n",
    "        return None\n",
    "\n",
    "    # 1. Transcription\n",
    "    try:\n",
    "        result = transcription_model.transcribe(audio_file, language=\"ar\")  # Arabic for Tunisian dialect\n",
    "        transcription = result[\"text\"]\n",
    "        print(f\"Transcription: {transcription}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Transcription failed: {e}. Using fallback text.\")\n",
    "        transcription = \"No transcription available\"\n",
    "\n",
    "    # 2. Stress Detection\n",
    "    try:\n",
    "        stress_input = torch.tensor(audio, dtype=torch.float32).unsqueeze(0)\n",
    "        stress_out = stress_model.encode_batch(stress_input, wav_lens=[1.0])\n",
    "        stress_prob = stress_out[0].softmax(dim=-1)\n",
    "        stress_label = stress_model.hparams.label_encoder.decode(stress_out[0].argmax().item())\n",
    "        stress_score = stress_prob.max().item()\n",
    "        # Boost stress score with text keywords\n",
    "        stress_words = [\"danger\", \"help\", \"run\", \"saha\", \"mousiba\", \"aidez-moi\"]\n",
    "        if any(word in transcription.lower() for word in stress_words):\n",
    "            stress_score = min(1.0, stress_score + 0.2)\n",
    "        print(f\"Stress Label: {stress_label}, Score: {stress_score:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Stress detection failed: {e}. Using default 'neutral' and 0.0 score.\")\n",
    "        stress_label, stress_score = \"neutral\", 0.0\n",
    "\n",
    "    # 3. Tone Analysis\n",
    "    try:\n",
    "        pitch, _ = librosa.pitches.melodia(audio, sr=sample_rate)\n",
    "        energy = np.sum(librosa.feature.rms(y=audio)**2)\n",
    "        tone_threshold = 0.5  # Adjust based on testing\n",
    "        tone = \"fearful\" if np.mean(pitch[~np.isnan(pitch)]) > tone_threshold or energy > tone_threshold else \"calm\"\n",
    "        print(f\"Tone: {tone}, Pitch Mean: {np.mean(pitch[~np.isnan(pitch)]):.2f}, Energy: {energy:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Tone analysis failed: {e}. Using default 'calm'.\")\n",
    "        tone = \"calm\"\n",
    "\n",
    "    # 4. Rhythm Analysis\n",
    "    try:\n",
    "        rhythm_out = rhythm_pipeline({\"waveform\": torch.tensor(audio).unsqueeze(0), \"sample_rate\": sample_rate})\n",
    "        speech_segments = rhythm_out.get_timeline().support()\n",
    "        total_duration = librosa.get_duration(y=audio, sr=sample_rate)\n",
    "        speech_duration = sum(seg.duration for seg in speech_segments)\n",
    "        speech_rate = len(speech_segments) / total_duration if total_duration > 0 else 0\n",
    "        pause_ratio = 1 - (speech_duration / total_duration) if total_duration > 0 else 0\n",
    "        rhythm = \"fast\" if speech_rate > 2.0 else \"slow\"\n",
    "        print(f\"Rhythm: {rhythm}, Speech Rate: {speech_rate:.2f} segments/s, Pause Ratio: {pause_ratio:.2f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Rhythm analysis failed: {e}. Using default 'slow'.\")\n",
    "        rhythm = \"slow\"\n",
    "\n",
    "    # 5. Alert Level\n",
    "    alert_level = \"High\" if stress_score > 0.7 or tone == \"fearful\" or rhythm == \"fast\" else \"Low\"\n",
    "    print(f\"Security Alert Level: {alert_level}\")\n",
    "\n",
    "    return {\n",
    "        \"transcription\": transcription,\n",
    "        \"stress_label\": stress_label,\n",
    "        \"stress_score\": stress_score,\n",
    "        \"tone\": tone,\n",
    "        \"rhythm\": rhythm,\n",
    "        \"alert_level\": alert_level\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b2b497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading audio: name 'librosa' is not defined\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLe noyau s’est bloqué lors de l’exécution du code dans une cellule active ou une cellule précédente. \n",
      "\u001b[1;31mVeuillez vérifier le code dans la ou les cellules pour identifier une cause possible de l’échec. \n",
      "\u001b[1;31mCliquez <a href='https://aka.ms/vscodeJupyterKernelCrash'>ici</a> pour plus d’informations. \n",
      "\u001b[1;31mPour plus d’informations, consultez Jupyter <a href='command:jupyter.viewOutput'>log</a>."
     ]
    }
   ],
   "source": [
    "\n",
    "# Example usage with a placeholder audio file\n",
    "# Since you have no data, record or download a sample WAV file\n",
    "audio_file = \"anger.wav\"  # Replace with actual file path\n",
    "results = analyze_vocal_audio(audio_file)\n",
    "if results:\n",
    "    print(\"Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dde78c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
