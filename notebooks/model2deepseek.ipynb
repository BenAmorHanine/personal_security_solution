{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b71a9648",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bca13c35",
   "metadata": {},
   "source": [
    "1. Transcription en Dialecte Tunisien\n",
    "\n",
    "Modèle ASR Recommandé :\n",
    "\n",
    "Tunisian_Automatic_Speech_Recognition (SalahZa) 6\n",
    "\n",
    "Performance : CER 6.22% / WER 10.55% sur TARIC\n",
    "\n",
    "Avantages :\n",
    "\n",
    "Traite le code-switching (Arabe/Français/Anglais)\n",
    "\n",
    "Modèle léger pour terminaux mobiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7ec990",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "asr_tn = pipeline(\"automatic-speech-recognition\", model=\"SalahZa/Tunisian_Automatic_Speech_Recognition\")\n",
    "transcription = asr_tn(\"audio.wav\")[\"text\"]  # Transcription en texte"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45df256b",
   "metadata": {},
   "source": [
    "Alternative Optimisée Mobile :\n",
    "\n",
    "LinTO ASR 10\n",
    "\n",
    "Usage :\n",
    "\n",
    "python\n",
    "from vosk import Model, KaldiRecognizer\n",
    "model = Model(\"linto-asr-ar-tn-0.1/android-model\")  # Version allégée\n",
    "recognizer = KaldiRecognizer(model, 16000)\n",
    "recognizer.AcceptWaveform(audio_data)\n",
    "transcription = json.loads(recognizer.FinalResult())[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b936d926",
   "metadata": {},
   "source": [
    "2. Détection de Stress Vocal (SER)\n",
    "Stack Technique :\n",
    "\n",
    "Framework : SpeechBrain 1\n",
    "\n",
    "Données d'entraînement :\n",
    "\n",
    "TunSwitch : 2631 phrases tunisiennes étiquetées 315\n",
    "\n",
    "CREMA-D : 7 442 clips avec 6 émotions (colère, peur, joie, etc.) 4\n",
    "\n",
    "MediaSpeech : Émotions en Arabe/Français 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "449466e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from speechbrain.pretrained import EncoderClassifier\n",
    "classifier = EncoderClassifier.from_hparams(\n",
    "    source=\"speechbrain/emotion-recognition\",\n",
    "    savedir=\"pretrained_models/emotion-recognition\"\n",
    ")\n",
    "# Adaptation avec données tunisiennes\n",
    "classifier.fit(\n",
    "    train_data=\"datasets/TunSwitch/train.csv\",\n",
    "    emotion_labels=[\"stress\", \"normal\", \"crise\"],\n",
    "    epochs=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c8b354",
   "metadata": {},
   "source": [
    "Extraction des Features Clés :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088cd363",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "def extract_stress_features(audio_path):\n",
    "    y, sr = librosa.load(audio_path)\n",
    "    f0 = librosa.pyin(y, fmin=80, fmax=400)[0]  # Hauteur vocale\n",
    "    rms = librosa.feature.rms(y=y)[0]            # Énergie sonore\n",
    "    return [np.nanmean(f0), np.std(f0), np.mean(rms)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f657ec7",
   "metadata": {},
   "source": [
    "3. Intégration Temps Réel\n",
    "   graph TD\n",
    "    A[Microphone] --> B(Transcription via SalahZa/LinTO)\n",
    "    A --> C(Détection Stress via SpeechBrain)\n",
    "    B --> D{Analyse Sémantique}\n",
    "    C --> E[Score de Stress]\n",
    "    D --> F[Mots-clés : “aide”, “danger”]\n",
    "    E & F --> G[Décision Alerte]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680d876e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "\n",
    "def audio_callback(indata, frames, time, status):\n",
    "    audio = indata[:, 0]\n",
    "    # 1. Transcription\n",
    "    transcript = asr_tn(audio)[\"text\"]\n",
    "    # 2. Détection stress\n",
    "    features = extract_stress_features(audio)\n",
    "    stress_score = classifier.predict(features)\n",
    "    # 3. Détection mots-clés\n",
    "    keywords = [\"secours\", \"danger\", \"aidez-moi\"]\n",
    "    alert = any(kw in transcript for kw in keywords) or stress_score > 0.8\n",
    "    if alert:\n",
    "        trigger_emergency(location=gps.get_location(), audio=audio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5971bb",
   "metadata": {},
   "source": [
    "puis il reste la conversion en lite"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
